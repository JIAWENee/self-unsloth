{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ä½¿ç”¨ Unsloth å®ç° qwen2.5-7B çš„æŒ‡ä»¤å¾®è°ƒ"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/unsloth/bin/python\n",
      "/root/Unsloth\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥çœ‹ Python çš„è·¯å¾„\n",
    "!which python\n",
    "\n",
    "# è·å–å½“å‰å·¥ä½œç›®å½•\n",
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# å®‰è£…ä¾èµ–\n",
    "!pip install unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/unsloth/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.518 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.32s/it]\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# å®šä¹‰æ¨¡å‹çš„æœ€å¤§åºåˆ—é•¿åº¦ï¼Œæ”¯æŒå†…éƒ¨çš„RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰æ‰©å±•\n",
    "# ä»…å½±å“å¾®è°ƒè¿‡ç¨‹ä¸­çš„è¾“å…¥é•¿åº¦ï¼Œå¾®è°ƒåçš„æ¨¡å‹çš„æœ€å¤§åºåˆ—é•¿åº¦ç”±åŸå§‹æ¨¡å‹çš„æ¶æ„å†³å®šã€‚\n",
    "max_seq_length = 2048\n",
    "\n",
    "# å®šä¹‰æ•°æ®ç±»å‹ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹ï¼›Float16é€‚ç”¨äºTesla T4ã€V100ï¼ŒBfloat16é€‚ç”¨äºAmpereæ¶æ„ã€‚\n",
    "dtype = None\n",
    "\n",
    "# æ˜¯å¦ä½¿ç”¨4ä½é‡åŒ–ï¼ˆ4bit quantizationï¼‰ä»¥å‡å°‘å†…å­˜ä½¿ç”¨ï¼ŒTrueè¡¨ç¤ºå¯ç”¨ã€‚\n",
    "load_in_4bit = True\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "# More models at https://huggingface.co/unsloth\n",
    "\n",
    "# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # Can select any from the below:\n",
    "    # \"unsloth/Qwen2.5-0.5B\", \"unsloth/Qwen2.5-1.5B\", \"unsloth/Qwen2.5-3B\"\n",
    "    # \"unsloth/Qwen2.5-14B\",  \"unsloth/Qwen2.5-32B\",  \"unsloth/Qwen2.5-72B\",\n",
    "    # And also all Instruct versions and Math. Coding verisons!\n",
    "    model_name = \"/root/Unsloth/models/Qwen2.5-7B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨LoRAï¼ˆä½ç§©é€‚é…ï¼‰æŠ€æœ¯å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒ\n",
    "ç°åœ¨æˆ‘ä»¬æ·»åŠ äº† LoRA é€‚é…å™¨ï¼Œå› æ­¤åªéœ€æ›´æ–°æ‰€æœ‰å‚æ•°ä¸­çš„ 1%åˆ° 10%ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Already have LoRA adapters! We shall skip this step.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,     # LoRAçš„ç§©ï¼Œé€‰æ‹©ä»»æ„ >0 çš„æ•°ï¼Œå»ºè®®å€¼ä¸º8ã€16ã€32ã€64ã€128ã€‚\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],    # æŒ‡å®šéœ€è¦åº”ç”¨LoRAçš„æ¨¡å—ã€‚\n",
    "    lora_alpha = 16,    # LoRA çš„ç¼©æ”¾å› å­ã€‚\n",
    "    lora_dropout = 0,   # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",      # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ•°æ®å‡†å¤‡\n",
    "æˆ‘ä»¬ç°åœ¨ä½¿ç”¨æ¥è‡ª [yahma çš„ Alpaca æ•°æ®é›†](https://huggingface.co/datasets/yahma/alpaca-cleaned)ï¼Œå®ƒæ˜¯åŸå§‹ Alpaca æ•°æ®é›† 52K çš„è¿‡æ»¤ç‰ˆæœ¬ã€‚æ‚¨å¯ä»¥ç”¨è‡ªå·±çš„æ•°æ®é¢„å¤„ç†æ›¿æ¢è¿™éƒ¨åˆ†ä»£ç ã€‚\n",
    "[æ³¨] è‹¥è¦åªå¯¹å®Œæˆåº¦è¿›è¡Œè®­ç»ƒï¼ˆå¿½ç•¥ç”¨æˆ·è¾“å…¥ï¼‰ï¼Œè¯·é˜…è¯»æ­¤å¤„çš„ TRL æ–‡æ¡£ã€‚\n",
    "[æ³¨æ„] è®°å¾—åœ¨æ ‡è®°åŒ–è¾“å‡ºä¸­æ·»åŠ  EOS_TOKENï¼å¦åˆ™ä¼šäº§ç”Ÿæ— é™ä»£ï¼\n",
    "å¦‚æœæ‚¨æƒ³å°† llama-3 æ¨¡æ¿ç”¨äº ShareGPT æ•°æ®é›†ï¼Œè¯·å°è¯•æˆ‘ä»¬çš„ä¼šè¯ç¬”è®°æœ¬\n",
    "å¯¹äºåƒå°è¯´å†™ä½œè¿™æ ·çš„æ–‡æœ¬è¡¥å…¨ï¼Œè¯·è¯•è¯•è¿™ä¸ªç¬”è®°æœ¬ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰Alpacaæ ¼å¼çš„æç¤ºæ¨¡æ¿\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "# è·å–åˆ†è¯å™¨çš„ç»“æŸæ ‡è®°ï¼ˆEOS_TOKENï¼‰ï¼Œç”¨äºæ ‡è®°ç”Ÿæˆçš„ç»“æŸã€‚\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "\n",
    "# å®šä¹‰æ ¼å¼åŒ–å‡½æ•°ï¼Œå°†æ•°æ®é›†ä¸­çš„æ ·æœ¬è½¬æ¢ä¸ºAlpacaæ ¼å¼\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "# åŠ è½½æ•°æ®é›†\n",
    "from datasets import load_dataset\n",
    "data_path = \"/root/Unsloth/dataset/alpaca-cleaned\"\n",
    "dataset = load_dataset(data_path, split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è®­ç»ƒæ¨¡å‹\n",
    "ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨ Huggingface TRL çš„ SFTTrainerï¼è¿™é‡Œæœ‰æ›´å¤šæ–‡æ¡£ï¼šTRL SFT æ–‡æ¡£ã€‚æˆ‘ä»¬ä½¿ç”¨ 60 æ­¥æ¥åŠ å¿«é€Ÿåº¦ï¼Œä½†ä½ ä¹Ÿå¯ä»¥è®¾ç½® num_train_epochs=1 æ¥è¿›è¡Œå®Œæ•´è¿è¡Œï¼Œå¹¶å…³é—­ max_steps=Noneã€‚æˆ‘ä»¬è¿˜æ”¯æŒ TRL çš„ DPOTrainerï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51760/51760 [00:06<00:00, 8114.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "# é…ç½®è®­ç»ƒå™¨\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "\n",
    "    # é…ç½®è®­ç»ƒå‚æ•°\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¼€å§‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 51,760 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 40,370,176/7,000,000,000 (0.58% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.796400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.710200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.901600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.971100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.882400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.665300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.981400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.909400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.848000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.704900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.839400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.711800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.962300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.720100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.702900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.876500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.221100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.899100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.714100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.752000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.844900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.857800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.873600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.929000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.926800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.965200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.815300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.805900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.690100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.812900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.774000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.915300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.711800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.881500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.761700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.734000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.636500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.006300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.013100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.825600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.886100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.824000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.832700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.902800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.881300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.714600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.157100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.819600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.026100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.967500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.846300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.938600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.070100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.732700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.976100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.815200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.717900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.766100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.827600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¨ç†\n",
    "è®©æˆ‘ä»¬è¿è¡Œæ¨¡å‹ï¼æ‚¨å¯ä»¥æ›´æ”¹æŒ‡ä»¤å’Œè¾“å…¥ - è¾“å‡ºç•™ç©ºï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nContinue the fibonnaci sequence.\\n\\n### Input:\\n1, 1, 2, 3, 5, 8\\n\\n### Response:\\n13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Continue the fibonnaci sequence.\", # instruction\n",
    "        \"1, 1, 2, 3, 5, 8\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ TextStreamer è¿›è¡Œè¿ç»­æ¨ç†--è¿™æ ·æ‚¨å°±å¯ä»¥é€ä¸ªçœ‹åˆ°ç”Ÿæˆçš„æ ‡è®°ï¼Œè€Œä¸ç”¨ä¸€ç›´ç­‰å¾…ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Continue the fibonnaci sequence.\n",
      "\n",
      "### Input:\n",
      "1, 1, 2, 3, 5, 8\n",
      "\n",
      "### Response:\n",
      "13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711, 25841, 41577, 67418, 109051, 176530, 285471, \n"
     ]
    }
   ],
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Continue the fibonnaci sequence.\", # instruction\n",
    "        \"1, 1, 2, 3, 5, 8\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä¿å­˜ã€åŠ è½½å¾®è°ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_model/tokenizer_config.json',\n",
       " 'lora_model/special_tokens_map.json',\n",
       " 'lora_model/vocab.json',\n",
       " 'lora_model/merges.txt',\n",
       " 'lora_model/added_tokens.json',\n",
       " 'lora_model/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨ï¼Œå¦‚æœæ‚¨æƒ³åŠ è½½æˆ‘ä»¬åˆšåˆšä¿å­˜çš„ç”¨äºæ¨ç†çš„ LoRA é€‚é…å™¨ï¼Œè¯·å°† False è®¾ä¸º Trueï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.518 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is a famous tall tower in Paris?\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "One of the most famous tall towers in Paris is the Eiffel Tower, which stands at 324 meters (1,004 feet) tall and was completed in 1889. It was designed by the French engineer Gustave Eiffel and has become one of the most recognizable landmarks in the world.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# alpaca_prompt = You MUST copy from above!\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"What is a famous tall tower in Paris?\", # instruction\n",
    "        \"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ Hugging Face çš„ AutoModelForPeftCausalLMã€‚åªæœ‰åœ¨æ²¡æœ‰å®‰è£… unsloth çš„æƒ…å†µä¸‹æ‰èƒ½ä½¿ç”¨ã€‚ç”±äºä¸æ”¯æŒ 4bit æ¨¡å‹ä¸‹è½½ï¼Œå®ƒçš„é€Ÿåº¦ä¼šæ…¢å¾—ä»¤äººç»æœ›ï¼Œè€Œ Unsloth çš„æ¨ç†é€Ÿåº¦æ˜¯å®ƒçš„ 2 å€ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.32s/it]\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    # I highly do NOT suggest - use Unsloth if possible\n",
    "    from peft import AutoPeftModelForCausalLM\n",
    "    from transformers import AutoTokenizer\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä¸º VLLM ä¿å­˜åˆ° float16\n",
    "æˆ‘ä»¬è¿˜æ”¯æŒç›´æ¥ä¿å­˜ä¸º float16ã€‚ä¸º float16 é€‰æ‹© merged_16bitï¼Œä¸º int4 é€‰æ‹© merged_4bitã€‚æˆ‘ä»¬è¿˜å…è®¸å°† lora é€‚é…å™¨ä½œä¸ºå¤‡ç”¨ã€‚ä½¿ç”¨ push_too_hub_merged ä¸Šä¼ åˆ°ä½ çš„ Hugging Face è´¦æˆ·ï¼æ‚¨å¯ä»¥ç™»å½• https://huggingface.co/settings/tokens è·å–ä¸ªäººä»¤ç‰Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GGUF / llama.cpp è½¬æ¢\n",
    "ä¸ºäº†ä¿å­˜ä¸º GGUF / llama.cpp æ ¼å¼ï¼Œæˆ‘ä»¬ç°åœ¨å·²åŸç”Ÿæ”¯æŒï¼æˆ‘ä»¬ä¼šå…‹éš† llama.cppï¼Œå¹¶é»˜è®¤ä¿å­˜ä¸º q8_0 æ ¼å¼ã€‚åŒæ—¶ä¹Ÿæ”¯æŒå…¶ä»–æ–¹æ³•ï¼Œæ¯”å¦‚ q4_k_mã€‚\n",
    "ä½¿ç”¨ save_pretrained_gguf æ–¹æ³•è¿›è¡Œæœ¬åœ°ä¿å­˜ï¼Œä½¿ç”¨ push_to_hub_gguf æ–¹æ³•ä¸Šä¼ åˆ° Hugging Faceã€‚\n",
    "\n",
    "ä»¥ä¸‹æ˜¯éƒ¨åˆ†æ”¯æŒçš„é‡åŒ–æ–¹æ³•ï¼ˆå®Œæ•´åˆ—è¡¨å¯å‚è€ƒæˆ‘ä»¬çš„ Wiki é¡µé¢ï¼‰ï¼š\n",
    "\n",
    "q8_0ï¼šå¿«é€Ÿè½¬æ¢ã€‚èµ„æºå ç”¨è¾ƒé«˜ï¼Œä½†é€šå¸¸å¯ä»¥æ¥å—ã€‚\n",
    "\n",
    "q4_k_mï¼šæ¨èä½¿ç”¨ã€‚å¯¹äºä¸€åŠçš„ attention.wv å’Œ feed_forward.w2 å¼ é‡ä½¿ç”¨ Q6_K é‡åŒ–ï¼Œå…¶ä½™ä½¿ç”¨ Q4_Kã€‚\n",
    "\n",
    "q5_k_mï¼šæ¨èä½¿ç”¨ã€‚å¯¹äºä¸€åŠçš„ attention.wv å’Œ feed_forward.w2 å¼ é‡ä½¿ç”¨ Q6_K é‡åŒ–ï¼Œå…¶ä½™ä½¿ç”¨ Q5_Kã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨ï¼Œåœ¨ llama.cpp ä¸­ä½¿ç”¨ model-unsloth.gguf æ–‡ä»¶æˆ– model-unsloth-Q4_K_M.gguf æ–‡ä»¶ï¼Œæˆ–è€…ä½¿ç”¨åŸºäº UI çš„ç³»ç»Ÿï¼Œä¾‹å¦‚ Jan æˆ– Open WebUIã€‚æ‚¨å¯ä»¥åœ¨è¿™é‡Œå®‰è£… Janï¼Œåœ¨è¿™é‡Œå®‰è£… Open WebUIã€‚\n",
    "\n",
    "å¤§åŠŸå‘Šæˆï¼å¦‚æœæ‚¨å¯¹ Unsloth æœ‰ä»»ä½•ç–‘é—®ï¼Œæ¬¢è¿è”ç³»æˆ‘ä»¬çš„ Discord é¢‘é“ï¼å¦‚æœæ‚¨å‘ç°ä»»ä½•é”™è¯¯ï¼Œæˆ–æƒ³äº†è§£æœ€æ–°çš„ LLM å†…å®¹ï¼Œæˆ–è€…éœ€è¦å¸®åŠ©ã€åŠ å…¥é¡¹ç›®ç­‰ï¼Œæ¬¢è¿åŠ å…¥æˆ‘ä»¬çš„ Discordï¼\n",
    "\n",
    "å…¶ä»–é“¾æ¥ï¼š\n",
    "\n",
    "è®­ç»ƒæ‚¨è‡ªå·±çš„æ¨ç†æ¨¡å‹ - Llama GRPO ç¬”è®°æœ¬ å…è´¹ Colab\n",
    "\n",
    "å°†å¾®è°ƒä¿å­˜åˆ° Ollamaã€‚å…è´¹ç¬”è®°æœ¬\n",
    "\n",
    "Llama 3.2 è§†è§‰å¾®è°ƒ - æ”¾å°„å­¦ç”¨ä¾‹ã€‚å…è´¹ Colab\n",
    "\n",
    "åœ¨æˆ‘ä»¬çš„æ–‡æ¡£ä¸­æŸ¥çœ‹ DPOã€ORPOã€æŒç»­é¢„è®­ç»ƒã€å¯¹è¯å¼å¾®è°ƒç­‰ç¬”è®°æœ¬ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
